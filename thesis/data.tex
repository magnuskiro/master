\chapter{Data, retrieval and structure}
% The data. What used, how and why. Acquired data how and from where. 

This section describes the data sources, methods for acquisition, and the
structure of the used data. \ref{data:tweets} describes twitter and the mined
tweets. \ref{data:dictionaries} describe the different lists of words used in
the classification process. And last the finance data is described in
\ref{data:finance}.

For each section the structure, characteristics, metadata and usage are
described. 

#TODO chapter outline
%

% The section about twitter and tweets 
\section{Tweets}\label{data:tweets}
A tweet is a massage posted on twitter. The message can be up to 140 characters
long and in many ways it resembles the well known
SMS\footnote{Wikipedia on Short Messaging Service:\url{https://en.wikipedia.org/wiki/Short_Message_Service}}.

Tweets are posted to the users profile. When other people posts a previously
posted tweet again it is called a retweet.

All users can follow other users on Twitter. Tweets from users you follow will
appear in you stream of tweets on the main page of twitter.
%

\subsection{Tweet Structure}
\paragraph{Structure}
\hspace{0pt}\\
There are a lot of metedata in the tweets. In fact most of the data in a tweet
object is metadata. 

The data we acquire from Twitter is in the JSON data format. JSON or \textit{JavaScript
Object Notation} is an open standard format that uses human-readable text to
transmit data objects consisting of attributeâ€“value
pairs\footnote{Wikipedia on JSON:\url{https://en.wikipedia.org/wiki/Json}}.


A positive thing about the JSON data format is that we can directly evaluate it
in python. By using literal evaluation in python the tweet object is
interpreted as a dict. This makes the use of a tweet easy.  

For an example of the data structure of a tweet, see appendix:
\ref{appendix:tweetStructure}.

\paragraph{Content}
\hspace{0pt}\\
A tweet is an astonishing compilation of data about who, where and when a tweet
was posted.

As for the content we have the text, the message itself. With the content we
have fields for all the links, all the emoticons, and all hashtags that are
present in a tweet. 

Every tweet is posted by a user. All the data of a user is also present for
each tweet. Here we have data on follower count, profile images, friend count,
time zone, and many other profile related items. 

For the sharing of a single tweet we have data fields such as favorite\_count
and user\_mentions. We also have favorited, retweet\_count. 

In addition we have the location of a given tweet. Where the tweet was posted,
the name of the place, the coordinates of the tweet, the country, and the id of
this place. 

See all the different metadata types in appendix: \ref{appendix:tweetStructure}.
%

% data acquisition
\subsection{Twitter API}
The twitter API is a convenient way for lots of people to access data from
twitter. Tweets, streams, timelines, profiles and more are available through
the api. 

To provide easy easy access and conformity to industry standards, the api
provides data in the JSON format. 

While the api does not give access to 100\% of the data from twitter, it gives
a good representation of the tweets from the last 7 days.
%

\paragraph{Setup}
\hspace{0pt}\\
To get access to the api there are a few requirements. You have to have a
twitter account. You have to register the application you are going to use the
api with, thereby getting authentication keys. Then you have to used the keys
authenticate with twitter before you access the api. 

For a simple guide to this we have
\url{http://datascienceandprogramming.wordpress.com/2013/05/14/twitter-api/} as
a good example.

The 4 authentication tokens you get from Twitter, app\_key, app\_secret, oauth\_token,a nd
oauth\_token\_secret, is used with the Twython
library\footnote{\url{https://pypi.python.org/pypi/twython/}} as described
below.

The simplest example of use of the api and the twython library can be described
as follows:
\begin{itemize}
	\item Authenticate towards twitter.
	\item Execute search query on twitter.
	\item Print ID's of all retrieved tweets.
\end{itemize} 

The code of the example is as follows:
\begin{listing}
\begin{verbatim}
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)
results = twitter.search(q='Search query', count='15')

for status in results['statuses']:
    print status['id']
\end{verbatim}
\end{listing}

For more advanced use we have generators, and lots of parameters and api
endpoints to use. Endpoints and search parameters will be described under
section \ref{data:twitter:endpoints}, \textit{API endpoints options}. 
The twyhon framework and its advanced usage can be explored more in the code and
in the documentation of the framework
\footnote{\url{https://twython.readthedocs.org/en/latest/usage/advanced_usage.html}}.
%

\paragraph{Restrictions}
\hspace{0pt}\\
#TODO API restrictions\\
request restrictions.
%

\paragraph{API endpoint options}\label{data:twitter:endpoints}
\hspace{0pt}\\
This is a list of the most useful endpoints and those used in the thesis.
\begin{itemize}
\item[q] A UTF-8, URL-encoded search query of 1,000 characters maximum, including
operators. Queries may additionally be limited by complexity.

\item[count] The amount of tweets acquired in each request. Standard = 15, max
= 100. 

\end{itemize}
%

\paragraph{Searching}
\hspace{0pt}\\

%

\paragraph{Mining optimization}
\hspace{0pt}\\
#TODO -rt, searching vs generator\\ 
%

\paragraph{Caveats}
\hspace{0pt}\\
Please note that Twitter's search service and, by extension, the Search API is
not meant to be an exhaustive source of Tweets. Not all Tweets will be indexed
or made available via the search interface.
%

\subsection{Tweet sets}
#TODO manual classification\\
#TODO search terms\\
#TODO limitations\\
%

\subsection{Biased Data}
#TODO write this section\\
Due to the necessity of a search term in the query, we only get tweets that are
related to the given terms.

Further more the datasets of manually labeled tweets are biased based on my
personal opinion and state of mind in the moment of classification.  

\subsection{Trend Data}
#TODO Briefly describe the mining and API shortcomings for this particular
use.\\
#TODO describe the trend search terms: '\_search-terms'\\
#TODO write shortcomings of the search terms. \\
#TODO Describe the tweet data sets and sorting. \\ 

\subsection{Problems, Shortcomings, and Possible Improvements}
The potential problems and shortcomings of the data. 

#TODO retweets.\\ 
#TODO search terms.\\
#TODO finance vs not finance.\\

% describing the dictionaries used in the classification of tweets. 
\section{Dictionaries}\label{data:dictionaries}
#TODO introduction to dictionaries, of corpus whatever the name. \\
#TODO the purpose of the dictionary\\
#TODO use of the dictionaries. \\
%

\subsection{Downloaded Dictionaries}
#TODO describe the distinctions of dl dict

\paragraph{Obama}
\hspace{0pt}\\
#TODO describe obama dictionary.\\ 

\paragraph{Loughran & McDonald}
\hspace{0pt}\\
#TODO describe this dictionary
Tim Loughran and Bill McDonald has a set of dictionaries available from the
websites of University of Notre Dame \footnote{#TODO fiks tekst: nd.edu:
\url{http://www3.nd.edu/~mcdonald/Word_Lists.html}}. 

List of Dictionaries:
\begin{itemize}
    \item negative words
General list of negative words. No particular category. Used for basic   
    \item positive words
This dictionary contains a small set of positive words. There are no general
category for the words. The words are not directly related to finance. 
    \item Uncertainty words
    \item litigious words
    \item modal words strong
    \item modal words weak
\end{itemize}
%

\subsection{Compiled Dictionaries}
The compiled dictionaries are based on two manually labeled tweet sets. My own,
the kiro dataset, and the obama tweet set.
 
#TODO ref the used datasets.\\ 

Details about the process of manually classifying tweets can be found in section
\ref{sentiment:manual_classification}.

#TODO describe the dictionary compilation.\\


List of dictionaries:

#TODO describe the different dictionaries\\ 
\begin{itemize}
    \item Obama original, Monogram 
	\subitem description

    \item LoughranMcDonald, Monogram 
	\subitem description

    \item Obama original and LoughranMcDonald, Monogram, combined
	\subitem description

    \item Kiro, Monogram, self compiled 
	\subitem description

    \item Obama, Monogram, self compiled 
	\subitem description

    \item Kiro, Bigram, self compiled 
	\subitem description

    \item Obama, Bigram, self compiled 
	\subitem description

    \item Kiro, Trigram, self compiled 
	\subitem description

    \item Obama, Trigram, self compiled 
	\subitem description
	\label{data:dictionary_list}
\end{itemize}
%

\subsection{Error analysis, removal of duplicate words}
When creating the different dictionaries we remove duplicates from the positive
and negative dictionary set. Words that are present in both the positive and
negative dictionary is removed. By doing this we remove words that has no
significance in the classification. But we also risk removing words with
significance.

When looking at the duplicate words from the monogram dictionary based on the
kiro dataset we found some errors.
As a selection of words found, we have \textit{dangerous, bad, go, inc, let, up, or, need, good, if, no, are, and, of, on, the,
is, as}.
Here we can see that the words \textit{good} and \textit{bad} are represented.
Which is not good. By removing the words from the dictionaries we have removed
significant words in further classification, thus reducing correctness of the
algorithm. This is one of the drawbacks of the monogram dictionaries.

When looking at the removed duplicate words for bigram and trigrams we found no
indication of the same problem. As the uniqueness of bigrams and trigrams are a
lot greater we end up with very few duplicates and only duplicates that has no
significance to the over all classification. Although we might have other
unknown problems.  

Most stop words and other insignificant words are removed with the removal of
duplicate words. The same thing cannot be said about the bigram and trigram
dictionaries. There we have no stop words present in themselves, but they are
frequently part of other terms. For further improvements of classification with
word counting and dictionary quality we should remove
stop words, such as \textit{as, is, on, off, and, or} etc, from the
tweet/sentence before creating bi- and trigrams.     
%

% The financial data used in the thesis. 
\section{Finance Data}\label{data:finance}
#TODO obtaining the data(potential mining operations)
#TODO about the dataset, csv
#TODO potential problems 
%

