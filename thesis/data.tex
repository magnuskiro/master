\chapter{Data, retrieval and structure}
We aim to describe the structure, the characteristics, the metadata and usage
for the data used. 

In this data chapter we have three sections. 

The section abut twitter and tweets, \ref{data:tweets}. Where we describe how we
use tweets, what tweets to use and how we acquire them.

Then we have a section describing the dictionaries, \ref{data:dictionaries}. How
we compile the dictionaries from tweets and how we use them. Their shortcomings
and the possible improvements.  

And last we have a section that describes the financial data that we use. Where
we get them, the structure and how the data is used. 
%

% The section about twitter and tweets 
\section{Tweets}\label{data:tweets}
A tweet is a massage posted on twitter. The message can be up to 140 characters
long and in many ways it resembles the well known
SMS\footnote{Wikipedia on Short Messaging Service:\url{https://en.wikipedia.org/wiki/Short_Message_Service}}.

Tweets are posted to the users profile. When other people posts a previously
posted tweet again it is called a retweet.

All users can follow other users on Twitter. Tweets from users you follow will
appear in you stream of tweets on the main page of twitter.
%

\subsection{Tweet Structure}
\paragraph{Structure}
\hspace{0pt}\\
There are a lot of metedata in the tweets. In fact most of the data in a tweet
object is metadata. 

The data we acquire from Twitter is in the JSON data format. JSON or \textit{JavaScript
Object Notation} is an open standard format that uses human-readable text to
transmit data objects consisting of attributeâ€“value
pairs\footnote{Wikipedia on JSON:\url{https://en.wikipedia.org/wiki/Json}}.


A positive thing about the JSON data format is that we can directly evaluate it
in python. By using literal evaluation in python the tweet object is
interpreted as a dict. This makes the use of a tweet easy.  

For an example of the data structure of a tweet, see appendix:
\ref{appendix:tweetStructure}.

\paragraph{Content}
\hspace{0pt}\\
A tweet is an astonishing compilation of data about who, where and when a tweet
was posted.

As for the content we have the text, the message itself. With the content we
have fields for all the links, all the emoticons, and all hashtags that are
present in a tweet. 

Every tweet is posted by a user. All the data of a user is also present for
each tweet. Here we have data on follower count, profile images, friend count,
time zone, and many other profile related items. 

For the sharing of a single tweet we have data fields such as favorite\_count
and user\_mentions. We also have favorited, retweet\_count. 

In addition we have the location of a given tweet. Where the tweet was posted,
the name of the place, the coordinates of the tweet, the country, and the id of
this place. 

See all the different metadata types in appendix: \ref{appendix:tweetStructure}.
%

% data acquisition
\subsection{Twitter API}
The twitter API is a convenient way for lots of people to access data from
twitter. Tweets, streams, timelines, profiles and more are available through
the api. 

To provide easy easy access and conformity to industry standards, the api
provides data in the JSON format. 

While the api does not give access to 100\% of the data from twitter, it gives
a good representation of the tweets from the last 7 days.
%

\paragraph{Setup}
\hspace{0pt}\\
To get access to the api there are a few requirements. You have to have a
twitter account. You have to register the application you are going to use the
api with, thereby getting authentication keys. Then you have to used the keys
authenticate with twitter before you access the api. 

For a simple guide to this we have
\url{http://datascienceandprogramming.wordpress.com/2013/05/14/twitter-api/} as
a good example.

The 4 authentication tokens you get from Twitter, app\_key, app\_secret, oauth\_token,a nd
oauth\_token\_secret, is used with the Twython
library\footnote{\url{https://pypi.python.org/pypi/twython/}} as described
below.

The simplest example of use of the api and the twython library can be described
as follows:
\begin{itemize}
	\item Authenticate towards twitter.
	\item Execute search query on twitter.
	\item Print ID's of all retrieved tweets.
\end{itemize} 

The code of the example is as follows:
\begin{listing}
\begin{verbatim}
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)
results = twitter.search(q='Search query', count='15')

for status in results['statuses']:
    print status['id']
\end{verbatim}
\end{listing}

For more advanced use we have generators, and lots of parameters and api
endpoints to use. Endpoints and search parameters will be described under
section \ref{data:twitter:endpoints}, \textit{API endpoints options}. 
The twyhon framework and its advanced usage can be explored more in the code and
in the documentation of the framework
\footnote{\url{https://twython.readthedocs.org/en/latest/usage/advanced_usage.html}}.
%

\paragraph{Restrictions}
\hspace{0pt}\\
The api has some access restrictions. Or rather rate limitations. This is to be
expected, as unlimited access would cripple the api.

Twitter limits request of a particular kind at 180 requests per 15 minutes.
Which means that we can do 180 searches spread evenly over the time interval. Or
we could do 180 requests as fast as possible and then wait. The rate limits can
be explored thoroughly in the twitter documentation\footnote{Twitter:
\url{https://dev.twitter.com/docs/rate-limiting/1.1}}

As for the practical implications  of the limitations. They are not a problem in
our case. We get more then enough data. By using a generator and pour out tweets
we get 1000 tweets in about 60 seconds. But then we have to wait 15 minutes.
This is suboptimal. As we will crash the program at access denied from the api.  
%

\paragraph{API endpoints}\label{data:twitter:endpoints}
\hspace{0pt}\\
Twitter has made a lot of different endpoints available. The endpoints are
divided into these categories: \textit{Timelines}, \textit{Tweets},
\textit{Search}, \textit{Streaming}, \textit{Direct Messages}, \textit{Friends
\& Followers}, \textit{Users}, \textit{Suggested Users}, \textit{Favorites},
\textit{Lists}, \textit{Saved Searches}, \textit{Places \& Geo},
\textit{Trends}, \textit{Spam Reporting}, \textit{OAuth}, and \textit{Help}.

Of these categories we mainly use \textit{OAuth}, \textit{Help} and
\textit{Search}.
Listing the endpoints used with parameters we get this list: 


\begin{table}
\centering
\label{tbl:twitterAPIendpoints}
\caption{Used Twitter API endpoints table}
\begin{tabular}{ r l | p{7cm} }
Category & Parameter & Description\\
\hline

\textit{search} & - & Used for acquisition of tweets. A query can take the
following parameters: \\
& q & A UTF-8, URL-encoded search query of 1,000 characters maximum,
including operators. Queries may additionally be limited by complexity. \\
& count & The amount of tweets acquired in each request. Standard =
15, max = 100. \\
& geocode & Get tweets close to these coordinates. \\
& lang & The language we want tweets in. Very limited by the number of people that speaks that language. \\
& locale & Language specific. If the query is in Norwegian we get tweets in Norwegian.\\
& result\_type & mixed, recent or popular. This is the general mix of tweets
returned in a search. Recent is the newest tweets, while popular are tweets that
are retweeted a lot and tweets from users with many followers. \\
& until & Gets tweets before the given time. \\
& since\_id & We get tweets posted after the given time. \\
& max\_id & We get tweets with ids lower then the one given. \\
& include\_entities & This parameter has no practical application to us. \\
& callback & An optional place where twitter can post tweets back to us. \\

\hline
\textit{authenticate} & - & How we login and get access to the api. \\
& oauth\_token & It is the authentication code or password to access
the api. \\

\hline
\textit{rate\_limit\_status} & - & The way we know if we are still inside
the request limitations. \\
& resources & The elements that we want the status of. Currently that
is search and help. \\ 

\end{tabular}
\end{table}
%

\paragraph{Searching}
\hspace{0pt}\\
To perform a search we use the parameters described in the \textit{API
endpoints} table in section \ref{tbl:twitterAPIendpoints}.

Mainly we need a query. The query is a normal search string where twitter will
find tweets containing all words in the string. 
\begin{verbatim}
query = "Finance Increase"
\end{verbatim}

Further we can expand the search by using logic. 
\begin{verbatim}
query = "Finance OR Investment AND Economy OR Growth"
\end{verbatim}

The specific search used to compile the tweetset the dictionaries are based on is the
following. Where we get all tweets containing one of the words: \textit{Finance}, \textit{Investment}, \textit{Economy}, and \textit{Growth}.
\begin{verbatim}
query = "Finance OR Investment OR Economy OR Growth"
\end{verbatim}

Adding other parameters such as count and language we can further improve our
search. To execute such a search we get python code like the following, where
'results' is a data structure containing tweets.
\begin{verbatim}
query = "Finance OR Investment OR Economy OR Growth"
results = twitter.search(q=query, count='15', language='no')
\end{verbatim}
%

\paragraph{Mining optimization}
\hspace{0pt}\\
The acquisition of tweets, or the mining operation went quite well. We got lots
of tweets. But we ran into a problem. Retweets. In some cases we got up to 90\%
retweets in a mining session. Many of the tweets being essentially the same
one, only retweeted multiple times. 

When we are using tweets to create dictionaries, we do not want duplicate data
that we do not need. Retweets are after all mostly duplicate data. So we removed
the retweets and got a lot more different tweets. 

As nearly all retweets start with 'RT' we can easily sort them out. Then we get
a query like this. 
\begin{verbatim}
query = "Finance OR Investment OR Economy OR Growth AND -RT"
\end{verbatim}

When using the twython framework and its cursor function we get a continuous
stream of tweets. A problem with this is that twython's cursor basically
executes multiple searches. Thus yielding the same tweets multiple times,
unless you change the search. So we change the search to accommodate this and
use the \textit{max\_id} parameter.
\begin{verbatim}
query = "Finance OR Investment OR Economy OR Growth"
results = twitter.cursor(
                twitter.search,
                q=query,
                count="100",
                lauage=language,
                max_id='the id of the last tweet')

for result in results:
    print result
\end{verbatim}
%

\paragraph{API Caveats}
\hspace{0pt}\\
The main caveats of the twitter API is the request limitations and the
limitations of the search engine. 

As twitter says themselves: 
'\textit{Please note that Twitter's search service and, by extension, the Search
API is not meant to be an exhaustive source of Tweets. Not all Tweets will be
indexed or made available via the search interface.}' and 
'\textit{The limitations of the search engine of twitter indexes only about a
weeks worth of tweets.}'\footnote{Twitter:
\url{https://dev.twitter.com/docs/faq#8650}}

Although this is not a big problem, coding and data acquisition could have been
simpler. The solution for the week limitation was to broaden the search to
include more words. This resulted in a more varied dataset, but also more
tweets. We initially wanted to analyse tweets related to finance, so this was a
bit of a negative point.  

Further caveats of the request limitations. This means that we have to mine
tweets over time. We should really have set this up on a server and mined
tweets with a cron job every 15 minutes\footnoe{Wikipedia on Cron:
\url{https://en.wikipedia.org/wiki/Cron}}.
%

\subsection{Tweet sets}
We ended up with two datasets to be used. The obama tweetset\footnote{Neal
Caren of University of North Carolina. Tweet file: \url{http://www.unc.edu/~ncaren/haphazard/obama_tweets.txt}}, which is a set of
tweets containing around 1300 tweets about obama and the election of 2008/2009.  
And a self compile dataset, referenced as the kiro dataset, based on the words: \textit{Finance},
\textit{Investment}, \textit{Economy}, and \textit{Growth}.

\paragraph{Search terms}
\hspace{0pt}\\
The kiro dataset is based on only four words. This is a very limited part of
twitter and in no way representative for it's full content. Neither does the
search terms represent a full range of finance words.

An improvement would use a wide variety of finance words to mine tweets. This
would improve the relation to finance and the relevance of the dictionaries
afterwords. 

\paragraph{Structure}
The self compiled datsets has one tweet per line. This is the JSON data object
that is automatically imported into python. 

As for the obama tweet set we only have the text, where we have one tweet text
per line. 

\paragraph{Caveats}
\hspace{0pt}\\
Obama tweets is not ideal for sentiment analysis. To much political nonsense to
comprehend. A political statement is not positive or negative. It is positive
in the eyes on some and negative in the eyes of others. And there are people
that think it is neutral because they do not care. Retweets are also present,
so the actual data we get out of the tweet set is limited. 

The kiro dataset has a lot retweets and neutral tweets in it. Therefore we have
only used positive and negative tweets later, ignoring all the neutral ones.
This gives us more relevant data to work with and less noise. Although we
should have used the neutral tweets to improve the dictionaries.
%

\subsection{Trend Data}
When mining larger sets of tweets, the rate limitations and week limitation is
a challenge. But solved by broadening the mining. The mining in itself is quite
easy. Just execute a search and store all the new unique tweets. 

To get a broad search we have a list of search terms. The terms are mostly
usernames, but also some hashtags and other words. A drawback with the search
terms are that they might not resemble the area of tweets we want to get. That
are the finance related terms. 

The trend tweets are stored in files named with the search term. Each term
having it's own file with tweets. Then we sort the tweets by date and get files
containing tweets for an individual date.

All the used search terms a stored in the file: '\_search-terms'
\footnote{Search term file:
\url{https://github.com/magnuskiro/master/blob/master/code/trend/_search-terms}}.
Most of the search terms are based on an article from 'Teknisk Ukeblad' where
they list the twitter handles that are most significant in the oil industry.
Since Norway has a lot of oil, the financial market and the trend is greatly
dependent on it. And we can see if there are any relations between the compiled
trend and the value of the stock exchange.  

\subsection{Problems, Shortcomings, and Possible Improvements}
Retweets are a source of concern. The retweets does not give much in the sense
of new and unique data. But they can provide a significance in sharing and
importance of a given tweet. Retweets should be investigated more thoroughly in
the future.  

A shortcoming of the data mining is the search terms. Are the terms
representative? Do we get good data or not? Are there other terms that are
better suited to get accurate results? There are to many questions to ask about
the data to rely on them to much. A wide array of tests and analysis should be
done to remove this factor as a problem. 

Another interesting point to consider is whether or not the choice of finance
as the are of focus was smart. Is this area more or less difficult then other
areas to navigate? We think that finance related tweets and news are more
objective and has a firmer answer. And in total have less emotion and less room
for interpretation.
%

% describing the dictionaries used in the classification of tweets. 
\section{Dictionaries}\label{data:dictionaries}

The dictionaries are lists of words used in the classification process. For
each set we have a list of positive words and a list of negative words. 

The dictionaries are self compiled or downloaded. The downloaded dictionaries
are very specific in their area. The LoughranMcDonald dictionaries only contain
financial terms, while the self compiled dictionaries contains everything. 

The purpose of the dictionaries are to provide a way to separate words. And
further to give a quantitative way to distinguish positive and negative tweets.
We also want to look at the quality of the different dictionaries. And the
method for dictionary compilation.  

We use the dictionaries to count mono-, bi-, and tri-grams.
And we can say something about the quality of the dictionaries and the method
for compilation.

The dictionaries are used in both classification methods. The simple
word\_count(\ref{sentiment:word_count_classification}) classification and the
more advanced classification using SVM(\ref{sentiment:svm_classification}) and
Naive bayes(\ref{sentiment:naive_bayes_classification}) classifiers. 
%

\subsection{Downloaded Dictionaries}
The downloaded dictionaries are dictionaries found on the Internet. They are
compiled by others, and their quality is questioned. The most significant
feature of the downloaded dictionaries are that they contain words form a
certain domain. The obama dictionary contains more words linked to politics,
while the LoughranMcDonald dictionary only has words from the financial domain.  

\paragraph{Obama}
\hspace{0pt}\\
The obama dictionary was created in relation to the obama tweet set
\footnote{Neal Caren of University of North Carolina. Tweet file:
\url{http://www.unc.edu/~ncaren/haphazard/obama_tweets.txt}}, and the us
presidential election of 2008.

In the positive list \footnote{Neal Caren of University of North Carolina.
Positive words: \url{http://www.unc.edu/~ncaren/haphazard/positive.txt}} there
are 2230 words. The frequency of political words over other types words are
uncertain. We have not done any analysis on this part. \textit{wisdom},
\textit{truthful}, \textit{profit}, and \textit{intact} are words found in
the positive list. This in itself speaks for the incorrectness of the
dictionary. Neither \textit{intact} nor \textit{wisdom} are words that in
themselves can be described as positive or negative. 

There are 3905 negative words in the list of negative words\footnote{Neal Caren
of University of North Carolina. Negative words:
\url{http://www.unc.edu/~ncaren/haphazard/negative.txt}}. 
The negative list include words such as \textit{decrease}, \textit{worried} and
\textit{tricky}. Duplicate words are also present, so some sort of improvement
of this dictionary should be done.  
%

\paragraph{Loughran & McDonald}
\hspace{0pt}\\
Tim Loughran and Bill McDonald has a set of dictionaries available from the
websites of University of Notre Dame \footnote{Bill McDonald, University of
Notre Dame:
\url{http://www3.nd.edu/~mcdonald/Word_Lists.html}}. 

The lists of words:\\
\begin{table}
\centering
\label{tbl:loughran_mcdonald_dictionaries}
\caption{LoughranMcDonald available dictionaries}
\begin{tabular}{ l p{9cm} }
Negative words & General list of negative words. \\
Positive words & General list of positive words. \\
Uncertainty words & Words like \textit{may}, \textit{maybe}, and
\textit{nearly}. Words that flag content to have no concrete sentiment.\\
Litigious words & Law related words, not much use for us. \\
Modal words strong & Strong descriptive words, such as \textit{Always}, and
\textit{Strongly}\\
Modal words weak & Weak words on moods, such as \texit{Somewhat}, and
\textit{Depends}. \\
\end{tabular}
\end{table}

The potential of these dictionaries are big, and much unused. We only use the
positive and negative lists of words. The other lists could be used for a
better measure of polarity or weighting of bigrams. The best use of this
dictionary is for comparison of the different dictionaries.  
%

\subsection{Compiled Dictionaries}
The compiled dictionaries are based on the two manually labeled tweet sets. The
kiro tweet set, and the obama tweet set.

Details about the process of manually classifying tweets can be found in section
\ref{sentiment:manual_classification}.

\paragraph{Dictionary Compilation}
\hspace{0pt}\\
The compilation of a dictionary is quite simple. We take a set of manually
labeled tweets, and extract words, then sort them into positive and negative.
\begin{itemize}
    \item 1: We import all the tweets to create dictionaries from. 
    \item 2: We take all the positive tweets and generate words from them.
Mono-, bi- or tri-grams. 
    \item 3: We repeat step 2 with the negative tweets. 
    \item 4: We remove words that are present in both the positive and negative
list. Removal of duplicate words. Resulting in two lists of unique words either
positive or negative.  
\end{itemize}

\paragraph{Characteristics}
For both datasets we compiled mono-, bi-, and trigrams. Giving us 6 sets of
compiled dictionaries to test. All dictionaries has the drawback of personal
bias. The mindset of the person labeling the dataset also effects the
dictionaries. We also do not consider stemming and stop words, because we
remove duplicate words.  
%

\subsection{List of dictionaries}
Table, listing all the different sets dictionaries used. There are a list of
negative and a list of positive words in each set. 

\begin{table}
\centering
\label{tbl:dictionaries}
\caption{Dictionary table}
\begin{tabular}{ p{5cm} p{7cm} }
Name of dictionary & Description \\
Downloaded:& \\
\hline
Obama original & Monograms, in relation to the obama tweet set. \\
LoughranMcDonald & Monograms, acquired from Bill McDonald's webpage.\\
Combined Obama original and LoughranMcDonald & Monogram. A combination of
the previous two dictionaries. \\

Compiled:& \\
\hline
Kiro, Monogram & Compiled from the kiro dataset. Containing
monograms. \\
Kiro, Bigram &  Containing terms consisting of two separate words. (\textit{an 
example}) \\
Kiro, Trigram & Containing terms consisting of three separate trigrams.
(\textit{example of trigram})\\
Obama, Monogram & Compile from the obama tweet set, monograms. \\
Obama, Bigram & Containing terms consisting of two separate words.
(\textit{another example}) \\
Obama, Trigram & Containing terms consisting of three separate trigrams.
(\textit{also an example})\\

	\label{data:dictionary_list}
\end{tabular}
\end{table}
%

\subsection{Error analysis, removal of duplicate words}
When creating the different dictionaries we remove duplicates from the positive
and negative dictionary set. Words that are present in both the positive and
negative dictionary is removed. By doing this we remove words that has no
significance in the classification. But we also risk removing words with
significance.

When looking at the duplicate words from the monogram dictionary based on the
kiro dataset we found some errors.
As a selection of words found, we have \textit{dangerous, bad, go, inc, let, up, or, need, good, if, no, are, and, of, on, the,
is, as}.
Here we can see that the words \textit{good} and \textit{bad} are represented.
Which is not good. By removing the words from the dictionaries we have removed
significant words in further classification, thus reducing correctness of the
algorithm. This is one of the drawbacks of the monogram dictionaries.

When looking at the removed duplicate words for bigram and trigrams we found no
indication of the same problem. As the uniqueness of bigrams and trigrams are a
lot greater we end up with very few duplicates and only duplicates that has no
significance to the over all classification. Although we might have other
unknown problems.  

Most stop words and other insignificant words are removed with the removal of
duplicate words. The same thing cannot be said about the bigram and trigram
dictionaries. There we have no stop words present in themselves, but they are
frequently part of other terms. For further improvements of classification with
word counting and dictionary quality we should remove
stop words, such as \textit{as, is, on, off, and, or} etc, from the
tweet/sentence before creating bi- and trigrams.     
%

% The financial data used in the thesis. 
\section{Finance Data}\label{data:finance}

\paragraph{Data acquisition}
\hspace{0pt}\\
Obtaining the financial data is easy. Point and click to receive a csv file
containing the data we need. \url{Netfonds.no} provides the content we need.

More specifically we get the data for the Oslo Stock exchange\footnote{Netfonds
data on OSEBX
\url{http://www.netfonds.no/quotes/paperhistory.php?paper=OSEBX.OSE&csv_format=csv}}. 

By creating a little script we get fresh data every time we run the script.
\begin{verbatim}
stock_exchange_history = urllib.urlopen(urli_of_datafile).readlines()
    for record in stock_exchange_history:
        # do tomething with the data
        print record
\end{verbatim} 

\paragraph{Structure}
\hspace{0pt}\\
The data file contains data back to 1997. Data for one day on each line. Only
the days the exchange have been open are present.
Fields in the csv file are: \textit{quote\_date},\textit{paper},
\textit{exch}, \textit{open}, \textit{high}, \textit{low}, \textit{close},
\textit{volume}, and \textit{value}.

\paragraph{Usage}
\hspace{0pt}\\
The values we use are: \textit{quote\_date}, \textit{close}, and
\textit{volume}.
A potential drawback of the data are the days that the exchange is closed. This
might complicate the coding of the trend comparison.
%

