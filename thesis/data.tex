\chapter{Data, retrieval and structure}\label{data}
We aim to describe the structure, the characteristics, the metadata and usage
for the data used. 

In this data chapter we have three sections. 

The section about twitter and tweets, \ref{data:tweets}, is where we describe
how we use tweets, what tweets to use and how we acquire them.

Then we have a section describing the dictionaries, \ref{data:dictionaries}. How
we compile the dictionaries from tweets and how we use them. Their shortcomings
and the possible improvements.  

And last we have a section that describes the financial data,
\ref{data:finance}, that we use. Where we get them, the structure and how the
data is used. 
%

% The section about twitter and tweets 
\section{Tweets}\label{data:tweets}
A tweet is a massage posted on twitter. The message in many ways resemble the
well known SMS\footnote{Wikipedia on Short Messaging
Service:\url{https://en.wikipedia.org/wiki/Short_Message_Service}}.

Tweets are posted to the users profile. When a user post an existing tweet
again, it is referred to as a retweet. 

All users can follow other users on Twitter. Tweets from users you follow will
appear in your stream of tweets on Twitter. The stream is the collection of
tweets from users you are following.  
%

\subsection{Tweet Structure}
\paragraph{Structure}
\hspace{0pt}\\
There are a lot of metedata in the tweets. In fact most of the data in a tweet
object is metadata. 

The data we acquire from Twitter is in the JSON data format. JSON or \textit{JavaScript
Object Notation} is an open standard format that uses human-readable text to
transmit data objects consisting of attributeâ€“value
pairs\footnote{Wikipedia on JSON:\url{https://en.wikipedia.org/wiki/Json}}.

For an example of the data structure of a tweet, see appendix
\ref{appendix:tweetStructure}.

\paragraph{Content}
\hspace{0pt}\\

A tweet is an astonishing compilation of data about who, where and when a tweet
was posted.

As for the content we have the text, the message itself. With the content we
have fields for all the links, all the emoticons, and all hashtags that are
present in a tweet. 

Hashtags:
\begin{verbatim}
u'hashtags': [
      {
        u'indices': [
          82,
          87
        ],
        u'text': u'G01V'
      },
      {
        u'indices': [
          88,
          95
        ],
        u'text': u'G01V11'
      }
\end{verbatim}


Every tweet is posted by a user. All the data of a user is also present for
each tweet. Here we have data on follower count, profile images, friend count,
time zone, and many other profile related items. 

For the sharing of a single tweet we have data fields such as 'favorite\_count'
and 'user\_mentions'. We also have 'favorited' and 'retweet\_count'. 

\begin{verbatim}
  u'in_reply_to_user_id': None,
  u'retweet_count': 0,
  u'id_str': u'390051769780142080',
  u'favorited': False,
  u'favorite_count': 0,
  u'user_mentions': [      
  ],
\end{verbatim}

In addition we have the location of a given tweet. Where the tweet was posted,
the name of the place, the coordinates of the tweet, the country, and the id of
this place. 

\begin{verbatim}
u'place': {
    u'full_name': u'Stavanger, Rogaland',
    u'url': u'https://api.twitter.com/1.1/geo/id/dee2255bd015b52c.json',
    u'country': u'Norway',
    u'place_type': u'city',
    u'bounding_box': {
      u'type': u'Polygon',
      u'coordinates': [
        [
          [
            5.5655417,
            58.884420999999996
          ],
          [
            5.8687141,
            58.884420999999996
          ],
          [
            5.8687141,
            59.0608787
          ],
          [
            5.5655417,
            59.0608787
          ]
        ]
      ]
    },
    u'contained_within': [
    ],
    u'country_code': u'NO',
    u'attributes': {
    },
    u'id': u'dee2255bd015b52c',
    u'name': u'Stavanger'
  },
\end{verbatim}

See all the different metadata types in appendix: \ref{appendix:tweetStructure}.
%

% data acquisition
\subsection{Twitter API}
The twitter API is a convenient way for lots of people to access data from
twitter. Tweets, streams, timelines, profiles and more are available through
the api. 

To provide easy access and conformity to industry standards, the api
provides data in the JSON format. 

While the api does not give access to 100\% of the data from twitter, it gives
a good representation of the tweets from the last 7 days.
%

\paragraph{Setup}
\hspace{0pt}\\
To get access to the api there are a few requirements. You must have a
twitter account. The application to be used has to be registered with Twitter.
Registering the application gives access to API keys. Then you have to used the
keys authenticate with twitter before you access the api. 

For a simple guide to this we have
\url{http://datascienceandprogramming.wordpress.com/2013/05/14/twitter-api/} as
a good example.

The 4 authentication tokens you get from Twitter, app\_key, app\_secret, oauth\_token,a nd
oauth\_token\_secret, is used with the Twython
library\footnote{\url{https://pypi.python.org/pypi/twython/}} as described
below.

A simple example of how to use the API with Twython is shown below. 
\begin{itemize}
	\item Authenticate towards twitter.
	\item Execute search query on twitter.
	\item Print ID's of all retrieved tweets.
\end{itemize} 

The code of the example is as follows:
\begin{python}
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)
results = twitter.search(q='Search query', count='15')

for status in results['statuses']:
    print status['id']
\end{python}

For more advanced use we have generators, and lots of parameters and api
endpoints to use. Endpoints and search parameters will be described under
section \ref{data:twitter:endpoints}, \textit{API endpoints options}. 
The twyhon framework and its advanced usage can be explored more in the code and
in the documentation of the framework
\footnote{\url{https://twython.readthedocs.org/en/latest/usage/advanced_usage.html}}.
%

\paragraph{Restrictions}
\hspace{0pt}\\
The api has some access restrictions. Or rather rate limitations. This is to be
expected, as unlimited access would cripple the api.

Twitter limits request of a particular kind at 180 requests per 15 minutes.
Which means that we can do 180 searches spread evenly over the time interval. Or
we could do 180 requests as fast as possible and then wait. The rate limits can
be explored thoroughly in the twitter documentation\footnote{Twitter:
\url{https://dev.twitter.com/docs/rate-limiting/1.1}}

As for the practical implications  of the limitations. They are not a problem in
our case. We get more then enough data. By using a generator and pour out tweets
we get 1000 tweets in about 60 seconds. But then we have to wait 15 minutes.
This is suboptimal. As we will crash the program at access denied from the api.  
%

\paragraph{API endpoints}\label{data:twitter:endpoints}
\hspace{0pt}\\
Twitter has made a lot of different endpoints available. The endpoints are
divided into these categories: \textit{Timelines}, \textit{Tweets},
\textit{Search}, \textit{Streaming}, \textit{Direct Messages}, \textit{Friends
\& Followers}, \textit{Users}, \textit{Suggested Users}, \textit{Favorites},
\textit{Lists}, \textit{Saved Searches}, \textit{Places \& Geo},
\textit{Trends}, \textit{Spam Reporting}, \textit{OAuth}, and \textit{Help}.

Of these categories we mainly use \textit{OAuth}, \textit{Help} and
\textit{Search}.
Listing the endpoints used with parameters we get this list: 


\begin{table}
\centering
\label{tbl:twitterAPIendpoints}
\caption{Used Twitter API endpoints table}
\begin{tabular}{ r l | p{7cm} }
Category & Parameter & Description\\
\hline

\textit{search} & - & Used for acquisition of tweets. A query can take the
following parameters: \\
& q & A UTF-8, URL-encoded search query of 1,000 characters maximum,
including operators. Queries may additionally be limited by complexity. \\
& count & The amount of tweets acquired in each request. Standard =
15, max = 100. \\
& geocode & Get tweets close to these coordinates. \\
& lang & The language we want tweets in. Very limited by the number of people that speaks that language. \\
& locale & Language specific. If the query is in Norwegian we get tweets in Norwegian.\\
& result\_type & mixed, recent or popular. This is the general mix of tweets
returned in a search. Recent is the newest tweets, while popular are tweets that
are retweeted a lot and tweets from users with many followers. \\
& until & Gets tweets before the given time. \\
& since\_id & We get tweets posted after the given time. \\
& max\_id & We get tweets with ids lower then the one given. \\
& include\_entities & This parameter has no practical application to us. \\
& callback & An optional place where twitter can post tweets back to us. \\

\hline
\textit{authenticate} & - & How we login and get access to the api. \\
& oauth\_token & It is the authentication code or password to access
the api. \\

\hline
\textit{rate\_limit\_status} & - & The way we know if we are still inside
the request limitations. \\
& resources & The elements that we want the status of. Currently that
is search and help. \\ 

\end{tabular}
\end{table}
%

\paragraph{Searching}
\hspace{0pt}\\
To perform a search we use the parameters described in the \textit{API
endpoints} table in section \ref{tbl:twitterAPIendpoints}.

Mainly we need a query. The query is a normal search string where twitter will
find tweets containing all words in the string. 
\begin{python}
query = "Finance Increase"
\end{python}

Further we can expand the search by using logic. 
\begin{python}
query = "Finance OR Investment AND Economy OR Growth"
\end{python}

The following query is the one used to create the original tweet sets. The
original tweet sets are later used as the basis for the dictionaries.
We get all tweets containing one of the words: \textit{Finance}, \textit{Investment}, \textit{Economy}, and \textit{Growth}.
\begin{python}
query = "Finance OR Investment OR Economy OR Growth"
\end{python}

Adding other parameters such as count and language we can further improve our
search. To execute such a search we get python code like the following, where
'results' is a data structure containing tweets.
\begin{python}
query = "Finance OR Investment OR Economy OR Growth"
results = twitter.search(q=query, count='15', language='no')
\end{python}
%

\paragraph{Mining optimization}
\hspace{0pt}\\
The acquisition of tweets, or the mining operation went quite well. We got lots
of tweets. But we ran into a problem. Retweets. In some cases we got up to 90\%
retweets in a mining session. Many of the tweets being essentially the same
one, only retweeted multiple times. 

When we are using tweets to create dictionaries duplicate data was a problem.
Retweets are mostly duplicate data, so we removed them to increase the quality
of the tweet set.

As nearly all retweets start with 'RT' we can easily sort them out. Then we get
a query like this. 
\begin{python}
query = "Finance OR Investment OR Economy OR Growth AND -RT"
\end{python}

When using the twython framework and its cursor function we get a continuous
stream of tweets. A problem with this is that twython's cursor basically
executes multiple searches. Thus yielding the same tweets multiple times,
unless you change the search. So we change the search to accommodate this and
use the \textit{max\_id} parameter.
\begin{python}
query = "Finance OR Investment OR Economy OR Growth"
results = twitter.cursor(
                twitter.search,
                q=query,
                count="100",
                lauage=language,
                max_id='the id of the last tweet')

for result in results:
    print result
\end{python}
%

\paragraph{API Caveats}
\hspace{0pt}\\
The main caveats of the twitter API is the request limitations and the
limitations of the search engine. 

As twitter says themselves: 
'\textit{Please note that Twitter's search service and, by extension, the Search
API is not meant to be an exhaustive source of Tweets. Not all Tweets will be
indexed or made available via the search interface.}' and 
'\textit{The limitations of the search engine of twitter indexes only about a
weeks worth of tweets.}'\footnote{Twitter:
\url{https://dev.twitter.com/docs/faq#8650}}

Although this is not a big problem, coding and data acquisition could have been
simpler. The solution for the week limitation was to broaden the search to
include more words. This resulted in a more varied dataset, but also more
tweets. We initially wanted to analyse tweets related to finance, so the
diversity of the dataset could be a problem. 

Further caveats of the request limitations. This means that we have to mine
tweets over time. We should really have set this up on a server and mined
tweets with a cron job every 15 minutes\footnote{Wikipedia on Cron:
\url{https://en.wikipedia.org/wiki/Cron}}.
%

\subsection{Tweet sets}
We ended up with two datasets to be used. The obama tweetset\footnote{Neal
Caren of University of North Carolina. Tweet file:
\url{http://www.unc.edu/~ncaren/haphazard/obama_tweets.txt}}, which is a set of
tweets containing around 1300 tweets about obama and the election of 2008/2009.
And a self compile dataset, referenced as the kiro dataset, based on the words:
\textit{Finance}, \textit{Investment}, \textit{Economy}, and \textit{Growth}. 

In future work it would be natural to connect the search for tweets towards
stocks in a better way. Another interesting thing would be to analyse how the
obama tweet set can be used to predict the stock market after the election. And
see if we could extract trends or other useful finance related information.  

\paragraph{Search terms}
\hspace{0pt}\\
The kiro dataset is based on only four words. This is a very limited part of
twitter and in no way representative for it's full content. Neither does the
search terms represent a full range of finance words.

An improvement would use a wide variety of finance words to mine tweets. This
would improve the relation to finance and the relevance of the dictionaries
afterwords. 

\paragraph{Structure}
\hspace{0pt}\\
The self compiled datsets has one tweet per line. This is the JSON data object
that is automatically imported into python. 

The obama tweet set only have the tweet text. We have one tweet per line. And
no metadata present. 

\paragraph{Caveats}
\hspace{0pt}\\
Obama tweets is not ideal for sentiment analysis. It has to much political
content to be easily comprehended. A political statement is not positive or
negative. It is positive in the eyes on some and negative in the eyes of others.
And there are people that think it is neutral because they do not care. Retweets
are also present, so the actual data we get out of the tweet set is limited. 

The kiro dataset has a lot retweets and neutral tweets in it. Therefore we have
only used positive and negative tweets later, ignoring all the neutral ones.
This gives us more relevant data to work with and less noise. Although we
should have used the neutral tweets to improve the dictionaries.
%

\subsection{Trend Data}\label{data:trend_data}
When mining larger sets of tweets, the rate limitations and week limitation is
a challenge. The mining in itself is quite easy. Just execute a search and store
all the new unique tweets. The problem appear when we need huge amounts of
tweets and a setup for mining continuously. 

Each request to Twitter can yield 100 tweets. This gives us a theoretical
maximum of 1800 tweets per 15 min. Then we have to factor in the fact that we
might already have the tweets we receive from Twitter. And that we might not
get 100 tweets per query. In addition the Twitter API only indexes tweets a
week back in time. Forcing us to mine tweets over a longer period of time.  

As we have not set up a server to do the mining for us we, we did it manually
every now and then. Resulting in very few tweets per day ratio. 
An example of output from our mining script:
\begin{verbatim}
@chevron
Info -- Found 18 new tweets
Info -- Metadata file created, containing 1627 tweets
\end{verbatim} 

To get a broad search we have a list of search words. The words are mostly
usernames, but also some hashtags and other words. A drawback with the search
words are that they might not resemble the area of research, finance.  
A subset of the search words are:
\begin{verbatim}
@annaljunggren         @industrienergi        $sto
@ap_energi_miljo       @industrienergiu       stock
@asmundaukrust         @iraqoil_gas           @svheikki
@bp_america            @janezpotocnikeu       @tageerlend
@chedegaardeu          @knebben               @_tekna_
@chevron               @kobbaen               @tekniskmuseum
decrease               @linetrezz             @terjeaa
@dn_no                 market                 @theoilcouncil
\end{verbatim}

The trend tweets are stored in files named with the search term. Each term
having it's own file with tweets. Before using the trend tweets they are sorted
by date. Example from the folder shows the structure of filenames. All the files
with 'trend-' prefix contain tweets sorted by date.   
\begin{verbatim}
economy.meta           @nikolaiastrup.meta    trend-Apr-20
@eiagov                @_nito_                trend-Apr-21
@eiagov.meta           @_nito_.meta           trend-Apr-22
@eirik_milde           @norskindustri         trend-Apr-23
@eirik_milde.meta      @norskindustri.meta    trend-Apr-24
@eirinolsen            @norskoljeoggass       trend-Apr-25
@eirinolsen.meta       @norskoljeoggass.meta  trend-Apr-26
@elonmusk              norway                 trend-Apr-27
@elonmusk.meta         norway.meta            trend-Apr-28
@energyindepth         @oeddep                trend-Apr-29
@energyindepth.meta    @oeddep.meta           trend-Apr-30
@enr_gop               @ogjonline             trend-May-01
@enr_gop.meta          @ogjonline.meta        trend-May-02
@erlendjordal          @oilandgasiq           trend-May-03
\end{verbatim}

All the used search terms a stored in the file: '\_search-terms'
\footnote{Search term file:
\url{https://github.com/magnuskiro/master/blob/master/code/trend/_search-terms}}.
Most of the search terms are based on an article\footnote{Teknisk Ukeblad:
\url{http://www.tu.no/petroleum/2014/04/05/dette-er-de-viktigste-twitrerne-for-oljebransjen}}
from 'Teknisk Ukeblad' where they list the twitter handles that are most
significant in the oil industry.

The majority of the Norwegian economy is based on oil and gas exports. This
means that other parts of the Norwegian economy is dependent on the oil
industry in many ways. As an example, we have all the service industry around
oil and gas, such as food supply, air travel, financial services etc. If the
oil industry has a hard time, all the dependent companies will have a hard time
to. 
 
This leads us to believe that OSEBX, the Oslo stock exchange, is largely
dependant on the oil industry also. Which makes our choice of tweets
reasonable. 
%

\subsection{Problems, Shortcomings, and Possible Improvements}
Retweets are a source of concern, they provide no information that is not
present in other tweets. But they can provide information about sharing and the importance of a given tweet. Retweets should be investigated more thoroughly in
the future.  

A shortcoming of the data mining is the search terms. Are the terms
representative? Do we get good data or not? Are there other terms that are
better suited to get accurate results? A wide array of tests and analysis should
be done to improve this factor of the problem. 
%

% describing the dictionaries used in the classification of tweets. 
\section{Dictionaries}\label{data:dictionaries}

The dictionaries are lists of words used in the classification process. There
are sets of dictionaries. In each set there are a list of positive words and a list of negative words. 

The dictionaries are self compiled or downloaded. The downloaded dictionaries
are very specific in their area. The LoughranMcDonald dictionaries only contain
financial terms, while the self compiled dictionaries contains words of all
sorts. 

The purpose of the dictionaries are to separate groups of words. And
further to give a quantitative way to classify positive and negative tweets.
We also want to look at the quality of the different dictionaries. And the
method for dictionary compilation.  

Monograms are words consisting of one word. 'This', 'That' and 'Finance' are
examples of that. Bigrams are words consisting of two words. 'Apple cake' and
'Operatin system' are two examples. For trigrams we have three words to a word,
'big blue boots', 'fast lane car', and 'waterfall boat accident'. 

We use the dictionaries to count mono-, bi-, and tri-grams.
And we can say something about the quality of the dictionaries and the method
for compilation.

The dictionaries are used in both classification methods. The simple
word\_count(\ref{sentiment:word_count_classification}) classification and the
more advanced classification using SVM(\ref{sentiment:svm_classification}) and
Naive bayes(\ref{sentiment:naive_bayes_classification}) classifiers. 
%

\subsection{Downloaded Dictionaries}
The downloaded dictionaries are dictionaries found on the Internet. They are
compiled by others, and their quality can be questioned. The most significant
feature of the downloaded dictionaries are that they contain words from a
certain domain. The obama dictionary contains words linked to politics,
while the LoughranMcDonald dictionary only has words from the financial domain.  

\paragraph{Obama}
\hspace{0pt}\\
The obama dictionary was created for the obama tweet set
\footnote{Neal Caren of University of North Carolina. Tweet file:
\url{http://www.unc.edu/~ncaren/haphazard/obama_tweets.txt}} in relation to the
us presidential election of 2008.

In the positive list \footnote{Neal Caren of University of North Carolina.
Positive words: \url{http://www.unc.edu/~ncaren/haphazard/positive.txt}} there
are 2230 words. Whether or not the frequency of political words are higher than
other types of words are uncertain. \textit{wisdom},
\textit{truthful}, \textit{profit}, and \textit{intact} are words found in
the positive list. Neither \textit{intact} nor \textit{wisdom} are words that
can be described as positive or negative. This gives a clear indication that
the dictionary can be improved.

There are 3905 negative words in the list of negative words\footnote{Neal Caren
of University of North Carolina. Negative words:
\url{http://www.unc.edu/~ncaren/haphazard/negative.txt}}. 
The negative list include words such as \textit{decrease}, \textit{worried} and
\textit{tricky}. Duplicate words are also present, so some sort of improvement
of this dictionary should be done.  
%

\paragraph{Loughran \& McDonald}
\hspace{0pt}\\
Tim Loughran and Bill McDonald has a set of dictionaries available from the
websites of University of Notre Dame \footnote{Bill McDonald, University of
Notre Dame:
\url{http://www3.nd.edu/~mcdonald/Word_Lists.html}}. 

\begin{table}
\centering
\label{tbl:loughran_mcdonald_dictionaries}
\caption{LoughranMcDonald available dictionaries}
\begin{tabular}{ l p{9cm} }
Negative words & General list of negative words. \\
Positive words & General list of positive words. \\
Uncertainty words & Words like \textit{may}, \textit{maybe}, and
\textit{nearly}. Words that flag content to have no concrete sentiment.\\
Litigious words & Law related words, not much use for us. \\
Modal words strong & Strong descriptive words, such as \textit{Always}, and
\textit{Strongly}\\
Modal words weak & Weak words on moods, such as \textit{Somewhat}, and
\textit{Depends}. \\
\end{tabular}
\end{table}

These dictionaries have a lot of potential. We only use the
positive and negative lists of words. The other lists could be used for a
better measure of polarity or weighting of bigrams. The best use of this
dictionary is for comparison of the different dictionaries.  
%

\subsection{Compiled Dictionaries}\label{data:compiled_dictionaries}
The compiled dictionaries are based on the two manually labeled tweet sets. The
kiro tweet set, and the obama tweet set.

Details about the process of manually classifying tweets can be found in section
\ref{sentiment:manual_classification}.

\paragraph{Dictionary Compilation}
\hspace{0pt}\\
The compilation of a dictionary is quite simple. 
\begin{itemize}
    \item 1: We import manually labeled the tweets
    \item 2: We take all the positive tweets and extract words from them. 
    \item 3: We repeat step 2 with the negative tweets. 
    \item 4: We remove words that are present in both the positive and negative
dictionaries. Removal of duplicate words. Resulting in two dictionaries of
unique words either positive or negative.  
\end{itemize}

\paragraph{Characteristics}
For both datasets we compiled mono-, bi-, and trigrams. Giving us 6 sets of
compiled dictionaries to test. All dictionaries has the drawback of personal
bias. The personality of the person labeling the dataset also effects the
dictionaries. 
%

\subsection{List of dictionaries}
Table \ref{tbl:dictionaries}, listing all the different dictionaries used. 
For each entry in the table there is a dictionary of negative words and a
dictionary of positive words. 

\begin{table}
\centering
\label{tbl:dictionaries}
\caption{Dictionary table}
\begin{tabular}{ p{5cm} p{7cm} }
Name of dictionary & Description \\
Downloaded:& \\
\hline
Obama original & Monograms, in relation to the obama tweet set. \\
LoughranMcDonald & Monograms, acquired from Bill McDonald's webpage.\\
Combined Obama original and LoughranMcDonald & Monogram. A combination of
the previous two dictionaries. \\

Compiled:& \\
\hline
Kiro, Monogram & Compiled from the kiro dataset. Containing
monograms. \\
Kiro, Bigram &  Containing terms consisting of two separate words. (\textit{an 
example}) \\
Kiro, Trigram & Containing terms consisting of three separate trigrams.
(\textit{example of trigram})\\
Obama, Monogram & Compile from the obama tweet set, monograms. \\
Obama, Bigram & Containing terms consisting of two separate words.
(\textit{another example}) \\
Obama, Trigram & Containing terms consisting of three separate trigrams.
(\textit{also an example})\\

	\label{data:dictionary_list}
\end{tabular}
\end{table}
%

% The financial data used in the thesis. 
\section{Finance Data}\label{data:finance}
The finance data consists of records from Oslo stock exchange, OSEBX, one
record for each opening day. 
The values we use are: \textit{quote\_date}, and \textit{close}.

\paragraph{Data acquisition}
\hspace{0pt}\\
Obtaining the financial data is easy. Point and click to receive a csv file
containing the data we need. \url{Netfonds.no} provides the content we need.

More specifically we get the data for the Oslo Stock exchange\footnote{Netfonds
data on OSEBX
\url{http://www.netfonds.no/quotes/paperhistory.php?paper=OSEBX.OSE&csv_format=csv}}. 

By creating a little script we get fresh data every time we run the script.
\begin{python}
stock_exchange_history = urllib.urlopen(urli_of_datafile).readlines()
    for record in stock_exchange_history:
        # do tomething with the data
        print record
\end{python} 

\paragraph{Structure}
\hspace{0pt}\\
The data file contains data back to 1997. Data for one day on each line. Only
the days the exchange have been open are present.
Fields in the csv file are: \textit{quote\_date},\textit{paper},
\textit{exch}, \textit{open}, \textit{high}, \textit{low}, \textit{close},
\textit{volume}, and \textit{value}.
%

