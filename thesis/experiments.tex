\chapter{Experiments}\label{experiments}
This chapter aims to test methods described in earlier chapters. We describe
setup and context for the experiments as well as the used data.

The experiments described in this chapter are the following: Dictionary
compilation, \ref{experiments:dictionaries}. The word count classification, bag
of words method, \ref{experiments:word_count_classification}. Threshold
variation in section \ref{experiments:threshold}. Kernel choice for the SVM
classifier is described in section \ref{experiments:svm_kernel}. The
testing of classifiers is found in section \ref{experiments:calssifiers}. And
trend aggregation in section \ref{experiments:trend}.

\section{Dictionary Compilation}\label{experiments:dictionaries}
Section \ref{data:dictionaries} describe dictionaries and what they are used
for. The dictionaries are compiled from manually labeled tweets.
This section describes how the dictionaries are tested. 

The dictionaries were created for the word count classification, 
as a step in the sentiment classification research.
To find which dictionaries performed well, or not, we tested them
with two data sets, the Kiro dataset and the Obama dataset. 

The quality of the dictionary is tested by removing duplicate words. This gives
an indication of the correctness of the dictionary. 
Words that are present in both the positive and
negative dictionary are removed. By doing this we remove words that has no
significance in the classification. But we also risk removing words with a 
clear sentiment. The code is described in appendix
\ref{code:remove_duplicate_words} on page \pageref{code:remove_duplicate_words}.

This experiment enlightens part of research question 1,
how we can find the sentiment of a tweet.

The quality of the dictionary is given by the number of correct classifications
it gives. The results of classification with each dictionary is described in
tabled \ref{tbl:classification_comparison}, on page
\pageref{tbl:classification_comparison}. 

Results from the testing of dictionaries are closely linked to the word count
classification in section \ref{experiments:word_count_classification} and shown in
table \ref{tbl:sentiment_word_count_results} on page
\pageref{tbl:sentiment_word_count_results}.
%

\section{Word Count Classification}\label{experiments:word_count_classification}
The background for the experiment and it's purpose is described in section
\ref{sentiment:classification}. This experiment used the technique described in
section \ref{sentiment:word_count_classification}, the word count method of
classifying a tweets sentiment.  

The main goal of this experiment is to determine the sentiment
of a tweet, section \ref{introduction:rq1}. And how Twitter can be used, indirectly, to
aggregate a trend, section \ref{introduction:rq2}.
The method looks at the number of positive and negative words and use
that to decide the sentiment we have.

Both datasets are classified with all the dictionaries. This
gives the accuracy for all dictionaries for the two datasets, and indicates the
efficiency of the word count classification. 

The accuracy of the classification is determined by looking at the number of
tweets that were classified correctly. The accuracy is the percentage of
correctly classified tweets.  

The code for calculating the sentiment is as follows. It is also described in
appendix \ref{code:word_count_classification}, on page
\pageref{code:word_count_classification}.
\begin{python}
positivity = positive_words_count / total_number_words
negativity = negative_words_count / total_number_words

polarity = positivity - negativity

if polarity > 0:
    tweet is positive
else:
    tweet is negative
\end{python}

The results of the classification can be found in section
\ref{results:word_count_classification} on page
\pageref{results:word_count_classification}.
%

\subsection{Threshold Variation}\label{experiments:threshold}
In section \ref{sentiment:threshold} the description and purpose of the
threshold variation is described. The threshold gives a degree of positivity. Or
rather how many more positive words than negative words we need to call the
tweet positive.  

By varying the threshold we hoped to find an optimal point where we could
separate tweets based on polarity. From the threshold graphs in figure
\ref{fig:threshold_graphs}, page \pageref{fig:threshold_graphs}, we can see no
clear distinction of one value being better than others.

What is done is that the word count classification is run multiple
times with varying threshold values. Values between -0.9 and 0.9. This gives a
distribution of results which is plotted in graph
\ref{fig:average_threshold_accuracy} on page
\pageref{fig:average_threshold_accuracy}.
The results gives an indication of what threshold is best for word counting. 

The results of the threshold variation is described in section
\ref{results:threshold}, on page \pageref{results:threshold}. And in table
\ref{tbl:average_threshold_accuracy} on page
\pageref{tbl:average_threshold_accuracy}.
%

\section{Using Classifiers}\label{experiments:calssifiers}
The two datasets was tested with the two trained classifiers.
This was done to find the best classifier.
We found the best classifier by comparing the results of the two trained
classifiers and the word count classification. 

Description of the comparison, on a conceptual level, is described in section
\ref{sentiment:comparison_results}. 
The experiment takes inspiration from concepts described in section
\ref{previous_work:sentiment}.

The research questions to be enlightened by the experiment is the trend
aggregation(RQ1) in section \ref{introduction:rq1} and the sentiment
classification(RQ2) in section \ref{introduction:rq2}.
%

\subsection{SVM}\label{experiments:svm_classification}

\paragraph{Choice of SVM Kernel}\label{experiments:svm_kernel}
\hspace{0pt}\\
In extension of section \ref{sentiment:svm_classification} all the SVM kernels were
tested.
While testing which kernel was best we used the Kiro dataset for training, and
the Kiro monogram dictionary for feature extraction.

Using the self compile monogram dictionaries and all the different SVM kernels
we get the results described in table \ref{tbl:svm_classifier_kernel_test}, page
\pageref{tbl:svm_classifier_kernel_test}.

\begin{table}
\centering
\label{tbl:svm_classifier_kernel_test}
\caption{SVM kernel test results table}
\begin{tabular}{ l r r c }
Kernel & Failed & Correct & Accuracy \\
\hline
LinearSVC & 7 & 990 & 0.9930 \\
NuSVC & 29 & 968 & 0.9709 \\
NuSVR & 422 & 575 & 0.5767 \\
OneClassSVM & 575 & 422 & 0.4233 \\
SVC & 422 & 575 & 0.5767 \\
SVR & 422 & 575 & 0.5767 \\
\end{tabular}
\end{table}

\paragraph{SVM Classification}
\hspace{0pt}\\
Results from testing SVM with different dictionaries are described in table
\ref{tbl:svm_classifier_results} on page
\pageref{tbl:svm_classifier_results}.

\begin{table}
\centering
\label{tbl:svm_classifier_results}
\caption{SVM classifier results table}
\begin{tabular}{ l l r r c }
Dataset & Type & Failed & Correct & Accuracy \\
\hline
Kiro & Monogram & 7 & 990 & 0.9930 \\
Kiro & Bigram & 422 & 575 & 0.5767 \\
Obama & Monogram & 35 & 1330 & 0.9744 \\
Obama & Bigram & 507 & 858 & 0.6286 \\
\end{tabular}
\end{table}

\subsection{Naive Bayes}\label{experiments:naive_bayes_classification}
\hspace{0pt}\\
Results from testing Naive Bayes with different dictionaries are described in
table \ref{tbl:naive_bayes_classification_results} on page
\pageref{tbl:naive_bayes_classification_results}.

\begin{table}
\centering
\label{tbl:naive_bayes_classification_results}
\caption{Naive Bayes classifier results table}
\begin{tabular}{ l l r r c }
Dataset & Type & Failed & Correct & Accuracy \\ 
\hline 
Kiro & Monogram & 29 & 968 & 0.9709 \\
Kiro & Bigram & 29 & 968 & 0.9709 \\
Obama & Monogram & 59 & 1306 & 0.9568 \\
Obama & Bigram & 59 & 1306 & 0.9568 \\
\end{tabular}
\end{table}
%

\section{Trend Aggregation}\label{experiments:trend}
Section \ref{trend:compared} describes the
conceptual parts of the trend comparison. This is the basis for the results of
this experiment. 

This experiment aims to test the relevance of positive and negative tweets in
correlation with change in finance. There are two parts to this experiment. The
finance part, and the twitter part. Can twitter be used to create a trend,
\ref{introduction:rq2}. And how can we compare this to a finance trend,
\ref{introduction:rq3}.

In the execution of the experiment the methods described in chapter \ref{trend}
has been used. More specifically the trend aggregation under 'trends on
twitter', section
\ref{trend:trends_on_twitter}. And the finance trend aggregation in
section \ref{trend:trends_in_finance}.

The tweet data used is described in section \ref{data:trend_data}, on page
\pageref{data:trend_data}. And the finance data is described in section
\ref{data:finance}, page \pageref{data:finance}.

The results of the experiment is discussed in section \ref{results:trend}. 
%
