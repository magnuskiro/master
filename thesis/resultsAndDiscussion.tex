\chapter{Results and Discussion}\label{results}
%All our results are in ones and zeroes. 
%And further we discuss why there are only zeroes. And how that affects the
%outcome and future endeavors for the pirates we are. 

In this chapter we present the results of our research and a concluding
discussion.
We discuss the core parts of the thesis. The data sources,
section \ref{results:data_sources}. Dictionaries, section
\ref{results:dictionaries}. Classification of sentiment in section
\ref{results:classification}, and the trend in section \ref{results:trend}. Last
we have a general discussion about the results.  
%

\section{Data Source}\label{results:data_sources}
The use of Twitter as a data source proved to be good. It was easy to obtain
data, but it was difficult to get good amounts of data and unbiased data. The
initial data have some restriction to search words. We
should have mined, and classified more data. 

The dictionaries worked well. We have different results for
different dictionaries and different datasets, but the way to
compile datasets work good. The dictionaries depend on the manually classified
tweets, so a lot of the potential improvement lies there. We can
improve the dictionaries by removing stop words and other words that we know
have no clear positive or negative sentiment. 

In research question 1, we asked what knowledge we can
extract from tweets to find a sentiment. 

The dictionaries are a good way of collecting information about sentiment, for
collections of manually labeled tweets. 
The combined sentiment of a dataset is in many ways described in a set of
dictionaries. 
 
Research question 3, asks if Twitter has been useful in
comparison to technical analysis. To some extent it has. 
The data from Twitter has been
used to acquire positive and negative tweets for the trend aggregation, and has
also provided useful insight into the use of the Twitter API. 
%

\section{Dictionaries}\label{results:dictionaries}
As for the use of the dictionaries in association with the trend, and research
question 3, we used the dictionaries for feature
extraction in the classifiers, previously described in section
\ref{sentiment:classifier_classification}. Besides the use in classifiers the
dictionaries have no practical uses in this thesis in relation to sentiment.

\paragraph{Error analysis}
\hspace{0pt}\\
When looking at the duplicate words\footnote{File With duplicate words:
\url{https://github.com/magnuskiro/master/blob/master/code/dictionaries/duplicate-words-from-monogram-compilation.txt}}
from the monogram dictionary based on the Kiro dataset we found some errors.
Some of the words found was: \textit{dangerous, bad, go, inc, let, up, or, need,
good, if, no, are, and, of, on, the, is, as}.

We found the words \textit{good} and \textit{bad}.
This is not good. By removing the words from the dictionaries we have removed
significant words from further classifications, thus reducing correctness of the
algorithm. This is one of the drawbacks of the monogram dictionaries.

When looking at the removed duplicate words for bigram and trigrams we found no
indication of the same problem. As the uniqueness of bigrams and trigrams are a
lot greater we end up with very few duplicates and only duplicates that has no
significance to the over all classification. Although we might have other
unknown problems.

Most stop words and other insignificant words are removed with the removal of
duplicate words. The same thing cannot be said about the bigram and trigram
dictionaries. There we have no stop words present, but they are
frequently part of other words. For further improvements of classification with
word counting, and dictionary quality we should remove
stop words, such as \textit{as, is, on, off, and, or} etc, from the
tweet/sentence before creating bi-, and trigrams.
%

\section{Classification}\label{results:classification}
Manual labeling is a challenge. The whole
classification process is based on personal opinion, which makes the results
somewhat biased. To eliminate this error we should have multiple people do the
same manual classification and combine the results.  

Word counting works to some extent. But presents varied results based on the
dataset and the dictionary used. This is because of the dataset and the
dictionaries created. Also the way we separate tweets from each other with a
threshold can be improved. Word counting works, but it is not very
good. 

Classifiers  improve the classification quite a bit. With the SVM classifier we
get around 98 percent correct classification on both data sets.

The uncertainties lie in the biased, or potentially biased, data. We do not
know if the data itself is fully representative for the content on Twitter.
Neither do we know if the manual classification is completely correct. 

In total we gained some knowledge about dictionaries and their effect on
classification. We can confirm that classifiers work better than word
counting.  

We tested two methods for finding the sentiment of a tweet: the bag of words
method described in section \ref{sentiment:word_count_classification}, and the
trained classifiers with the SVM and Naive Bayes classifiers,
\ref{sentiment:classification}.

In relation to research question 2, the question about trend
aggregation, we found that the sentiment classification was successful. This is
shown in tabled \ref{tbl:classification_comparison}, on page
\pageref{tbl:classification_comparison}. 
The sentiment worked to some extent. We do not
know how well, and we have to look into the part of credibility in future work.   
%

\subsection{Word Count Classification}\label{results:word_count_classification}
The results from the word count classification is plotted in the tabled
\ref{tbl:sentiment_word_count_results}, page
\pageref{tbl:sentiment_word_count_results}, and in the graph
\ref{fig:dictionary_accyracy}, page \pageref{fig:dictionary_accyracy}.

\begin{table}
\centering
\label{tbl:sentiment_word_count_results}
\caption{Word Count Classification Results}
\begin{tabular}{ r p{6cm} r r c }
id & Dictionary & Failed & Correct & Accuracy \\
& -- Kiro compiled dataset -- & a & b & b/(a+b) \\
\hline
1 & Monogram, Obama & 578 & 419 & 0.4203 \\
2 & Monogram LoughranMcDonald & 491 & 506 & 0.5075 \\
3 & Monogram, combined Obama and LoughranMcDonald & 416 & 581 & 0.5827 \\
4 & Kiro, Monogram, self compiled & 115 & 882 & 0.8847 \\
5 & Kiro, Bigram, self compiled & 17 & 980 & 0.9829 \\
6 & Kiro, Trigram, self compiled & 18 & 979 & 0.9819 \\
7 & Obama, Monogram, self compiled & 567 & 430 & 0.4313 \\
8 & Obama Bigram, self compiled & 534 & 463 & 0.4644 \\
9 & Obama Trigram, self compiled & 567 & 430 & 0.4313 \\
& -- Obama tweet set -- & a & b & b/(a+b) \\
\hline
10 & Monogram, Obama & 855 & 510 & 0.3736 \\
11 & Monogram LoughranMcDonald & 508 & 857 & 0.6278 \\
12 & Monogram, combined Obama and LoughranMcDonald & 544 & 821 & 0.6015 \\
13 & Kiro, Monogram, self compiled & 632 & 733 & 0.5370 \\
14 & Kiro, Bigram, self compiled & 521 & 844 & 0.6183 \\
15 & Kiro, Trigram, self compiled & 498 & 867 & 0.6352 \\
16 & Obama, Monogram, self compiled & 493 & 872 & 0.6388 \\
17 & Obama Bigram, self compiled & 37 & 1328 & 0.9729 \\
18 & Obama Trigram, self compiled & 39 & 1326 & 0.9714 \\
\end{tabular}
\end{table}

In table \ref{tbl:sentiment_word_count_results}, page
\pageref{tbl:sentiment_word_count_results}, id 5, 6, 17, and 18 are the
dictionaries with best accuracy. This is expected for the classification
of the dataset the dictionary was created from. 

We take the results for dictionaries created form one dataset,
and look at the results of the classification on the other dataset. From table
\ref{tbl:sentiment_word_count_results} we compared id
7, 8, 9 with each other and 13, 14, 15 with each other. Then we can see that
the bigram and trigram dictionaries perform better than the monogram
dictionaries.

Further we see indications that the quality of the dictionary plays an
important role. The LoguhranMcDonald monogram dictionary performs quite good
with both datasets. Even on par with the compiled dictionaries on opposite
dataset. This indicates that a well crafted dictionary can perform as well
as compiled dictionaries, and vice versa.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{dictionary_accyracy.png}
    \label{fig:dictionary_accyracy}
    \caption{Dictionary Accuracy plot}
The graph shows the plotted accuracies from the word count
classification. The ids represents the ids from table
\ref{tbl:sentiment_word_count_results}, page
\pageref{tbl:sentiment_word_count_results}
\end{figure}

When comparing the results from one dataset with the other we can see indications
of how the content of the dataset also plays a huge role in the results. Some
tweets are more difficult to classify then others.

There are a lot of improvements that could be done.
The word count classification is merely a convenient way to say something about
the dictionaries used. We should expand our knowledge toward the quality of the
dictionaries. And look for ways to eliminate words that are neither positive or
negative. 

\subsection{Threshold Variation}\label{results:threshold}
Further we found the best average threshold value to be 0.1.
From table \ref{tbl:average_threshold_accuracy}, page \pageref{tbl:average_threshold_accuracy}, we have the threshold value, and the average
classification accuracy among the 18 entries for each threshold value.

\begin{table}
\centering
\label{tbl:average_threshold_accuracy}
\caption{Average Threshold Accuracy}
\begin{tabular}{ c c c c }
Threshold & Accuracy & Threshold & Accuracy \\
\hline
- & - & 0.0 & 0.6479 \\
-0.1 & 0.6316 & 0.1 & 0.6516 \\
-0.2 & 0.6161 & 0.2 & 0.6511 \\
-0.3 & 0.6059 & 0.3 & 0.6430 \\
-0.4 & 0.5988 & 0.4 & 0.6305 \\
-0.5 & 0.5888 & 0.5 & 0.6122 \\
-0.6 & 0.5711 & 0.6 & 0.5934 \\
-0.7 & 0.5423 & 0.7 & 0.5712 \\
-0.8 & 0.5083 & 0.8 & 0.5457 \\
-0.9 & 0.4881 & 0.9 & 0.5307 \\
\end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{average_threshold_accuracy.png}
    \label{fig:average_threshold_accuracy}
    \caption{Average Threshold Accuracy}
Graph plots of the 'Average threshold accuracy' table.
\end{figure}

In figure \ref{fig:threshold_graphs}, page \pageref{fig:threshold_graphs}, we list the results of the experimentation
with the threshold. Table \ref{tbl:dictionary_to_threshold}, page
\pageref{tbl:dictionary_to_threshold}, lists the
dictionaries and dataset used for which graphs in figure
\ref{fig:threshold_graphs}, page \pageref{fig:threshold_graphs}.
'Kiro dataset' and 'Obama dataset' columns tells which dataset that was
classified in which graph.

\begin{table}
\centering
\label{tbl:dictionary_to_threshold}
\caption{Dictionary to Threshold graph plot values}
\begin{tabular}{ l c c }
Dictionary name and description & Kiro dataset & Obama dataset \\
\hline
Obama original, Monogram & 1 & 10 \\
LoughranMcDonald, Monogram & 2 & 11 \\
Combined Obama original and \\ LoughranMcDonald, Monogram & 3 & 12 \\
Kiro, Monogram, self compiled & 4 & 13 \\
Obama, Monogram, self compiled & 5 & 14 \\
Kiro, Bigram, self compiled & 6 & 15 \\
Obama, Bigram, self compiled & 7 & 16 \\
Kiro, Trigram, self compiled & 8 & 17 \\
Obama, Trigram, self compiled & 9 & 18 \\
\end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{threshold_graphs.png}
    \label{fig:threshold_graphs}
    \caption{Threshold Variation Accuracy}
The graphs plot the different variations of threshold. Counting is
columns first; top left is 1, top mid is 7, top right is 13.
\end{figure}

\subsection{Using Classifiers}
Not surprisingly the classifiers have better results than the bag of words
method of classification, see table \ref{tbl:classification_comparison}, page
\pageref{tbl:classification_comparison}. Further the SVM classifier with the LinearSVC kernel
module performed best. Naive Bayes classifier was nearly as good. This answers
part of research question 1, \ref{introduction:rq1}.

\paragraph{SVM}\label{results:svm_classification}
\hspace{0pt}\\
From results of the experiment in section \ref{experiments:svm_classification},
we can draw some conclusions. The monogram dictionary performed
better than the bigram dictionary. Probably due to the number of features we
can extract from each tweet. Classifiers are more accurate than the word
count classification method.

\paragraph{Naive Bayes}\label{results:naive_bayes_classification}
\hspace{0pt}\\
As we can see from the experiment, section
\ref{experiments:naive_bayes_classification}, the different dictionaries makes
no difference for the Naive Bayes classifier. But if we compare the classifier
with the results from the word count classification we can clearly see
improvement. Naive Bayes did not perform as good as SVM. 

\subsection{Comparison}\label{results:comparison}
In table, \ref{tbl:classification_comparison}, page
\pageref{tbl:classification_comparison}, we highlight the
results from the different classifications. We list the most noteworthy results
from our experiments and compare them.

\begin{table}
\centering
\label{tbl:classification_comparison}
\caption{Comparison of Classifiers}
Here we highlight the best results from classification tests. 
\begin{tabular}{ l l p{3.5cm} r r c }
Classifier & Dataset & Dictionary & Failed & Correct & Accuracy \\
\hline
Word Count & Kiro & Obama Bigram & 534 & 463 & 0.4644 \\
Word Count & Obama & LoughranMcDonald Monogram & 508 & 857 & 0.6278 \\
Word Count & Obama & Kiro, Trigram & 498 & 867 & 0.6352 \\
Naive Bayes & Kiro & Kiro Monogram & 29 & 968 & 0.9709 \\
SVM & Kiro & Kiro Monogram & 7 & 990 & 0.9930 \\
\end{tabular}
\end{table}

As we can see the classifiers give better results then the word count
classification. And SVM is a bit better than Naive Bayes. Although the results
from the word count classification indicates that the dictionaries play an
important role in the results. We can also see that monograms are better for
classifiers, while trigrams are better for word counting.

For the trained classifiers, the more
features we have the easier it is to classify, and the more accurate the
results are.
%

\section{The Trend}\label{results:trend}
Comparing the two graph plots, figure \ref{fig:trend_finance_plot}, page
\pageref{fig:trend_finance_plot}, and figure
\ref{fig:trend_tweet_plot}, page \pageref{fig:trend_tweet_plot}, we can see minor similarities. All the moving
averages have rightward growth. While for the tweet trend the 3 day interval
preforms worse than the 8 day interval. There are no certain correlations
between the two trends. And it is difficult to conclude on specifics. 

Research questions(RQ), 2(\ref{introduction:rq2}), and
3(\ref{introduction:rq3}) are in some degree true. RQ2, we can aggregate a
trend, the use for it and the accuracy we know nothing of. RQ3 has the
answer 'poor'. The sentiment trend compares poorly to the trend from the technical
analysis. 

\paragraph{Finance plot}
\hspace{0pt}\\
The plotting of the graph is described in better detail in appendix
\ref{code:trend_aggregation}, page \pageref{code:trend_aggregation}.

Plotting the finance trend we got figure \ref{fig:trend_finance_plot}, page
\pageref{fig:trend_finance_plot}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{finance_trend_plot.png}
    \label{fig:trend_finance_plot}
    \caption{Finance trend plot}
This is the plot for OSEBX. The plot show the moving average in the middle and
the Average directional graph at the bottom. Time frame: Apr 26 - May 26.

The blue line in the top part shows the moving average with 8 days as the
average time frame. The white line has a 3 day time frame.

The bottom part show three lines, red, green, and white. The red is reduction
in price. The green is increase in price. The white one is the ADX.
\end{figure}


\paragraph{Twitter plot}
\hspace{0pt}\\
Code for plotting the trend graph is described in appendix
\ref{code:trend_aggregation}, page \pageref{code:trend_aggregation}.

Plotting the moving average for the Twitter trend we get figure:
\ref{fig:trend_tweet_plot}, on page
\pageref{fig:trend_tweet_plot}.

\begin{figure}[htb]
	\centering
    \includegraphics[width=\textwidth]{tweet_trend_plot.png}
    \label{fig:trend_tweet_plot}
    \caption{Tweet trend plot}
This is the Twitter trend plot. The plot show the moving average in the middle
and the Average directional graph at the bottom. Time frame: Apr 26 - May 26.

The blue line in the top part shows the moving average with 8 days as the
average time frame. The white line has a 3 day time frame.

The bottom part show three lines, red, green, and white. The red is reduction
in price. The green is increase in price. The white one is the ADX.
\end{figure}
%

\section{Discussion}\label{results:discussion}
We have satisfying and unsatisfying results. The
reasonable results should be expanded, retested and improved. 
There are clear indications that dictionaries, automatically compiled
from a manually classified tweets, can be successfully used to classify
sentiment.

The accomplished research, based on the RQ, can be described
as satisfactory. We set out to investigate how we can determine the sentiment of
a tweet, \ref{introduction:rq1}, how Twitter can be used to aggregate a trend,
\ref{introduction:rq2}, and how Twitter based trends compare to the technical
analysis of stock markets, \ref{introduction:rq3}. 

RQ1 more specifically aims at the information of tweets, the knowledge we
can extract from them, and classification of the sentiment of a tweet. By
creating dictionaries we extracted words from manually labeled tweets, giving us
a way to classify new tweets. We ended up only using the tweet text for
classification. Other metadata should have been explored. The best way of
classifying tweets we found to be the SVM classifier. We have gained new
knowledge in most aspects of RQ1. 
 
RQ2 investigates the complicated part of aggregating a trend based on Twitter. This
turned out to be difficult. Mainly by the lack of good ideas of how we could do
it, but also the difficulty of transforming data to a usable format. In the end
we transformed tweet data to the same format as stock data and used that for
the trend aggregation. 

RQ3 takes the trend a step further and compares technical analysis with the
Twitter based sentiment trend. After plotting MA and ADX for both finance and
Twitter data, we compared the graphs. The two plots
can be seen in figure \ref{fig:trend_finance_plot}, page
\pageref{fig:trend_finance_plot}, and figure \ref{fig:trend_finance_plot}, page
\pageref{fig:trend_finance_plot}. In the figures we looked for similarities.
For the MA we can argue that the blue lines, MA with 8
day intervals, are quite similar. We can make no definite conclusion
that the Twitter trend is representative for the finance market. Technical
analysis is still better, but sentiment analysis is a possibility. 
The ADX show no similarities between the Twitter sentiment trend and the finance
trend. 
%
