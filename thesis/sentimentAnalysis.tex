\chapter{Sentiment Classification}\label{sentiment}

Sections in this chapter are as follows. Manual classifications,
\ref{sentiment:manual_classification}, where we see how people classify
tweets. Classification with classifiers follows in section
\ref{sentiment:classification}. Containing 'Word count classification',
\ref{sentiment:word_count_classification}, and classification with classifiers
such as SVM and Naive Bayes in section
\ref{sentiment:classifier_classification}. A comparison of the classifiers and
associated results can be found in section \ref{sentiment:comparison_results}, and a brief discussion come last, in section
\ref{sentiment:comments_discussion}. 

%what is sentiment
Sentiment is described as "an attitude toward something; regard;
opinion."\footnote{ Dictionary.com on Sentiment:
\url{http://dictionary.reference.com/browse/sentiment?s=t}}. The sentiment is the perceived positivity of the message that the user tries to
communicate. Sentiment is in many cases a personal thing, and can change from
person to person or from setting to setting. We think of the sentiment as a
conveyed meaning of a message. 

%why do we get it
Some of the motivation for acquiring the sentiment of a tweet or a sentence, is
that we can say something about a persons state of mind and from that predict
behaviour. Further the aspect of sentiment in trading is interesting. Can
sentiment analysis improve trading in any way? And how can the sentiment be used
to predict trends.

%how do we use it
In this thesis we have two main ways of classifying tweets. Word counting and
training a classifier. Both methods require dictionaries of positive and
negative words, \ref{data:dictionaries}. In the classifier we use the dictionary
to extract features from a tweet. And with the word counting we count the
number of positive and negative words. 

\section{Manual Classification}\label{sentiment:manual_classification}
When labeling tweets manually there are a number of factors that affects the
process. Among them are the quality of the tweet, state of mind of the
classifier, language, and political affiliation.

The content describes the quality of the tweet in many ways. Does the tweet
contain links or hashtags? Are users mentioned?

State of mind for a person dictates that persons actions in many cases. This
also has an effect on labeling tweets. A person with a positive state of mind
classifies more negative tweets as positive.

Note that all this happens in the brain during 3 to 60 seconds while reading
the tweet text and is a very simple description of the process.
Labeling a tweet follows this algorithm in many ways:

\begin{tabular}{ l p{3cm} p{6cm} }
Step & Thought & Description \\
1 & Have we seen this tweet before? & Skip it or use previous classification. \\
2 & \#Hashtags or links present? & Some hashtags are automatically positive or
negative. Also remove noise such as users and links.\\
3 & Sarcasm? & If sarcasm is present, put up a warning flag saying that it is
the opposite of step 4.\\
4 & Special words? & Find a word that triggers positive or negative
impression.\\
5 & Done & Label tweet as positive or negative.\\
\end{tabular}

\paragraph{Result files}
\hspace{0pt}\\
The results file from the manual classification are comma separated variable
files(csv) with three fields.
\begin{itemize}
    \item Sentiment: Positive, neutral or negative. Represented by 1, 0 or -1.
    \item Tweet id, if it exists, else 'id'. It is a long number.
    \item The tweet text.
\end{itemize}

\paragraph{Obame tweet set}
\hspace{0pt}\\
When labeling the Obame tweet set we found that there were many retweets in the
dataset. Whether or not the data is representative for Twitter in general is
difficult to say. Tweets favoring Obame are positive for people who support
Obame and negative for supporters of Romney. This makes the
tweet set difficult to work with because tweets can be positive and negative at
the same time, depending on who are reading them. 

\paragraph{Kiro tweet set}
\hspace{0pt}\\
The kiro tweet set also has a lot of retweets, but they are not used later.
Removing the retweet increased the uniqueness of the data, as the same content
do not show up multiple times. The search terms the dataset is based on
could be broader to increase the diversity of the tweets.
%

\section{Classification}\label{sentiment:classification}
\subsection{Word Count Classification}\label{sentiment:word_count_classification}
We describe the classification process and the different parts of it. The
results and discuss section looks at the drawbacks and results of this method.
The algorithm counts the positive and negative words. More positive words than
negative means the tweet is positive and vice versa. 

\paragraph{Polarity} 
\hspace{0pt}\\ 
The polarity of a given tweet is based on the difference in the amount of
positive versus negative words. 

Polarity is the difference between positive and negative words. The difference is
expressed as the percentage of the total numbers of words.  

We look at the difference between the negative and the positive word
percentage. If the difference is positive we have a positive tweet, and if the
difference is negative we have a negative tweet.

\paragraph{Drawbacks}
\hspace{0pt}\\
There are some drawbacks to the word count classification, the dictionaries
and the threshold. The threshold is described in section
\ref{sentiment:threshold}. 

The threshold gives inaccuracy because tweets that have a polarity value that
is the same as, or close to, the threshold might be classified wrong. 

The quality of the dictionaries affects the word count classification. If the
dictionaries had better quality we would get better results. An example of
improvements would be to remove stop words before creating bi and monograms.

\subsection{Threshold in Word Count Classification}\label{sentiment:threshold} 
\hspace{0pt}\\ 

Threshold where tweets are either positive or negative. It is compared
to the polarity value. For a tweet to be classified as positive the ratio of
positive and negative words has to be above the threshold value. 

The percentage of positive words minus the percentage of negative words gives
the polarity value, or the positivity(how positive a tweet is) of a tweet. 
When actually deciding if a tweet is positive or negative we look at the
polarity value. If the polarity value is greater than the threshold, the tweet is classified as positive. 

The threshold is given to the classifier on initialization. We find which
threshold is best in section \ref{experiments:threshold}, on page 
\pageref{experiments:threshold}.

\paragraph{Examples of classification} 
\hspace{0pt}\\ 
Example tweets:
\begin{itemize}
    \item t1 = “good that he was decreasing badly”
    \item t2 = “he was good for increase” 
    \item t3 = “good or bad”
\end{itemize}

Classification of t1:
\begin{itemize}
	\item positive words: 'good' 
    \item pos = 1 / 6 = 0.16666
	\item negative words: 'decreasing', 'badly'
    \item neg = 2 / 6 = 0.33333
    \item polarity = pos - neg = -0.1667
    \item threshold of 0 gives negative classification
    \item threshold of 0.1 gives negative classification
    \item threshold of -0.2 gives positive classification
\end{itemize}

Classification of t2:
\begin{itemize}
	\item positive words: 'good', 'increase'
    \item pos = 2 / 5 (to av fem ord) = 0.4
    \item neg = 0 / 5 = 0
	\item negative words: none
    \item polarity = pos - neg = 0.4 - 0.0 = 0.4
    \item threshold = 0.4 -- positive
    \item threshold = 0.5 -- negative 
    \item threshold = -0.1 -- positive
\end{itemize}

Classification of t3:
\begin{itemize}
	\item positive words: 'good'
    \item pos = 1 / 3 = 0.3333
	\item negative words: 'bad'
    \item neg = 1 / 3 = 0.3333
    \item polarity = pos - neg = 0
    \item threshold = 0 -- positive
    \item threshold = 0.1 -- negative
    \item threshold = -0.1 -- positive
	\item 
\end{itemize}

\paragraph{Drawbacks with the Threshold}
\hspace{0pt}\\
When classifying with the word count classifier a problem occurs when we
get an equal amount of positive and negative words. Then the polarity value
becomes 0. This results in a situation where we have no indication of a tweet
being positive or negative. This is an area where we can improve the
classification. 

\begin{table}
\centering
\label{tbl:word_counting_polarity_null}
\caption{Word Count results where Threshold value=0 table}
\begin{tabular}{ r p{6cm} r c }
id & Dictionary & Polarity=0 & Tweets \\
& -- Kiro compiled dataset -- & & \\
\hline
1 & Monogram, Obame & 234 & 997 \\ 
2 & Monogram LoughranMcDonald & 543 & 997 \\ 
3 & Monogram, combined Obame and LoughranMcDonald & 178 & 997 \\
4 & Kiro, Monogram, self compiled & 53 & 997 \\
5 & Kiro, Bigram, self compiled & 7 & 997 \\
6 & Kiro, Trigram, self compiled & 28 & 997 \\
7 & Obame, Monogram, self compiled & 14 & 997 \\
8 & Obame Bigram, self compiled & 446 & 997 \\
9 & Obame Trigram, self compiled & 931 & 997 \\

& -- Obame tweet set -- & & \\
\hline
10 & Monogram, Obame & 335 & 1365 \\ 
11 & Monogram LoughranMcDonald & 854 & 1365 \\
12 & Monogram, combined Obame and LoughranMcDonald & 345 & 1365 \\
13 & Kiro, Monogram, self compiled & 233 & 1365 \\
14 & Kiro, Bigram, self compiled & 462 & 1365 \\
15 & Kiro, Trigram, self compiled & 1221 & 1365 \\
16 & Obame, Monogram, self compiled & 37 & 1365 \\
17 & Obame Bigram, self compiled & 52 & 1365 \\
18 & Obame Trigram, self compiled & 92 & 1365 \\
\end{tabular}
\end{table}

The amount of tweets where the threshold an polarity are equal is shown in table
\ref{tbl:word_counting_polarity_null} on page \pageref{tbl:word_counting_polarity_null}. The number of cases where we
get an equal amount of positive and negative words are significantly reduced if
the threshold is set to 0.1. We can also see indications that the diversity and
quality of the dictionary plays a role in the classification correctness. The
larger the dictionary the more likely it is that we do not get an equal
amount of positive and negative words.  
%

\subsection{Using Classifiers}\label{sentiment:classifier_classification}
To test and compare classification methods we used Naive Bayes and
Support Vector Machine classifiers. 

Comparing the two classifiers we found that SVM best. Although we see quite a
span within the SVM results. Details of the results of the two classifiers are
described in section \ref{experiments:svm_classification}. 

The self compiled dictionaries based on the kiro dataset was used to compare
the two types of classifiers. 

Each test with a classifier uses the kiro monogram dictionary for feature
extraction. Both datasets are used in testing.
Feature extraction is the process where we find elements of a tweet that is
relevant for the sentiment.  

\paragraph{SVM}\label{sentiment:svm_classification}
\hspace{0pt}\\
Support Vector Machine \footnote{Wikipedia:
\url{https://en.wikipedia.org/wiki/Support_vector_machine}}, is a classification
algorithm that uses a supervised learning model. The supervised learning is used
with associated learning algorithms to recognize patterns and analyse data. 

SVM builds a model, based on the training data, that split the data into two
categories. New data will be assign one of the two categories. This makes the
classifies a non-probabilistic binary linear classifier. 

In more specific terms SVM creates multiple hyperplanes. The larger the
distance from the hyperplane to the training data, the better the
classification. The hyperplanes can be used for regression, classification or
other tasks. 

When using SVM there are different kernel modules that can be used
to improve results. The kernel is often chosen based on knowledge about the
dataset. In the nltk\footnote{Natural Language Toolkit
\url{http://www.nltk.org/}} library for python there are a number of different
kernels. 

The kernels were tested to find out which kernel gave the best results. Section
\ref{experiments:svm_kernel} describes the experiment and the results. As
seen in the experiment the LinearSVC kernel performed best and was used in
further experiments.
%

\paragraph{Naive Bayes}\label{sentiment:naive_bayes_classification}
Naive Bayes \footnote{Wikipedia:
\url{https://en.wikipedia.org/wiki/Naive_Bayes}}, is based on Bayes theorem. It
is a simple probabilistic classifier. It uses strong(naive) independence
assumptions with the theorem.

The classifier assumes that the features are independent. That they have no
connection to each other and that all features have equal importance. 
A conditional model is used for with the probability model. Abstractly we get
this: 

\math{
p(C \vert F_1,\dots,F_n)
}\\

In plain English we have the reformulated model that can handle larger datasets:

\math{
\mbox{posterior} = \frac{\mbox{prior} \times \mbox{likelihood}}{\mbox{evidence}} 
}\\

Even more specifically the implementation in nltk that we use are: 
\begin{python}
                   P(label) * P(f1|label) *...* P(fn|label)
P(label|features)=-----------------------------------------
                   SUM[l]( P(l) * P(f1|l) *...* P(fn|l) )
\end{python}

Naive Bayes is efficient to train in a setting with supervised
learning. It requires only a small training set. The classifier have worked well
with complex real-world problems. Good results despite simplicity have been
found theoretically sound. But Naive Bayes is still outperformed by algorithms
such as 'random forests'.
%

\section{Comparison}\label{sentiment:comparison_results}
\paragraph{The Classifiers}
\hspace{0pt}\\
We have looked at three methods of classifying, Naive Bayes classifier, SVM
classifier, and the word count classifier.
Together they have classified the two dataset we have with varying results. 

All methods of classification uses the manually labeled tweet sets as the data
source. We use the self compile dictionaries for feature extraction.

\paragraph{Experiments}
\hspace{0pt}\\
In section \ref{sentiment:word_count_classification},
\ref{experiments:threshold}, \ref{experiments:svm_kernel}, and
\ref{experiments:calssifiers} the methods described in this chapter are tested
to validate the research questions, \ref{introduction:research_questions}.

\paragraph{Results}
\hspace{0pt}\\
The results of the classifiers are described in \ref{results:comparison}. There
the good and bad parts of the classifiers are presented and discussed. 
%

\section{Comments}\label{sentiment:comments_discussion}
\paragraph{Improvements}
\hspace{0pt}\\
There are a lot of improvements that could be done. 

The word count classification is merely a convenient way to say something about
the dictionaries used. We should expand our knowledge toward the quality of the
dictionaries, and look for ways to eliminate terms that are neither positive or
negative. 

We should cross classify and test the classification with the SVM classifier and
do quality assurance on the work with it. We should also look at other
classifiers and other data to confirm the findings we have so far. 

\paragraph{Biased Mind}
\hspace{0pt}\\
The datasets of manually labeled tweets are biased, based on a personal opinion
and the state of mind in the moment of classification. Therefore we have to keep
in mind that all the results are based on the assumption that everyone agrees on
the manual labeling. This is of course a source of errors to be
explored more in \ref{future_work}. 

We should explore the psychology of perception and classification. How do
people perceive content differently? Is finance easier to label than politics?

\paragraph{Drawbacks}
\hspace{0pt}\\
The quality of the dictionaries is a problem. Some of our observations give
insight into the use of natural language in association with dictionaries. 
Further drawbacks are the threshold of the word count classification. This
should be addressed in some smart way. 

Code wise we should improve the testing of the classifiers, and remove the
static use of dictionaries in the feature extraction.  
%
